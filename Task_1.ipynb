{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMC0Lw13DQGJtuoFXsulDk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadBinTariq/ATML_PA0/blob/main/Task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Task 1: Inner Workings of ResNet-152"
      ],
      "metadata": {
        "id": "usxN4HyupBVd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt0wQ7qso06h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "FdfhTKewpH9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Setup\n",
        "\n",
        "- Pre-trained ResNet-152\n",
        "- Replace final classification layer to match smaller datasets e.g. CIFAR-10\n",
        "- Train the head while freezing backbone\n",
        "- Record training and validation performance for a few epochs"
      ],
      "metadata": {
        "id": "LhOBirZ_pKoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-trained Model\n",
        "model_resnet = torchvision.models.resnet152(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Replacing model head\n",
        "# print(model_resnet)\n",
        "# print(model_resnet.fc)\n",
        "model_resnet.fc = nn.Linear(2048, 10)\n",
        "model_resnet.to(device)\n",
        "\n",
        "# Freezing backbone layers\n",
        "for param in model_resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# # Unfreeze classification head for training\n",
        "for param in model_resnet.fc.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "uiBUyXpupIzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e04de6-4a97-4d0e-b8cc-168e2e5c4d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230M/230M [00:01<00:00, 176MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function and Optimizer\n",
        "resnet_loss_fn = nn.CrossEntropyLoss()\n",
        "resnet_optimizer = torch.optim.SGD(\n",
        "    filter(lambda p: p.requires_grad, model_resnet.parameters()),\n",
        "    lr=0.01,\n",
        "    momentum=0.9\n",
        ")\n",
        "\n",
        "# Training and Testing Loops\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    running_loss, correct = 0.0, 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        pred = model(images)\n",
        "        loss = loss_fn(pred, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        correct += (pred.argmax(1) == labels).type(torch.float).sum().item()\n",
        "\n",
        "    train_loss = running_loss / size\n",
        "    accuracy = correct / size\n",
        "    return train_loss, accuracy\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.eval()\n",
        "    running_loss, correct = 0.0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            pred = model(images)\n",
        "            loss = loss_fn(pred, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            correct += (pred.argmax(1) == labels).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss = running_loss / size\n",
        "    accuracy = correct / size\n",
        "    return test_loss, accuracy"
      ],
      "metadata": {
        "id": "JfnXoSjEpMoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the CIFAR-10 Dataset\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "resnet_train_transforms = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "resnet_test_transforms = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "resnet_trainset = datasets.CIFAR10(root='./CIFAR10', train=True, download=True, transform=resnet_train_transforms)\n",
        "resnet_testset = datasets.CIFAR10(root='./CIFAR10', train=False, download=True, transform=resnet_test_transforms)\n",
        "\n",
        "resnet_trainloader = DataLoader(resnet_trainset, batch_size=64, shuffle=True)\n",
        "resnet_testloader = DataLoader(resnet_testset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "whREaKJIpOMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc8b777-5327-449b-a147-5f87f8800ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:07<00:00, 22.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Validation Performance\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train(resnet_trainloader, model_resnet, resnet_loss_fn, resnet_optimizer)\n",
        "    test_loss, test_acc = test(resnet_testloader, model_resnet, resnet_loss_fn)\n",
        "    print(f\"Epoch {epoch+1}: Train Acc {train_acc:.4f}, Test Acc {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y1tWqirpQex",
        "outputId": "0c6d825d-c6d0-426f-a228-d024f20dfa96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Acc 0.7808, Test Acc 0.8004\n",
            "Epoch 2: Train Acc 0.8170, Test Acc 0.8348\n",
            "Epoch 3: Train Acc 0.8228, Test Acc 0.8364\n",
            "Epoch 4: Train Acc 0.8339, Test Acc 0.8324\n",
            "Epoch 5: Train Acc 0.8361, Test Acc 0.8434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why is it unnecessary (and impractical) to train ResNet-152 from scratch on small datasets? What does freezing most of the network tell us about the transferability of features?\n",
        "\n",
        "- One of the main reasons is overfitting. A small dataset may not capture all characteristics of the space our data belongs to. This leads to the model memorizing the training data and poor generalization overall.\n",
        "- ResNet-152 has approximately 60 million model parameters. Training such a deep model requires compute resource and is time intensive (as seen from our own experiments also; it took ~9 minutes per epoch with the backbone frozen).\n",
        "- The fact that CIFAR-10 dataset is small and the images are 32x32, we are inherently using a \"canon to shoot a mosque\" with ResNet-152.\n",
        "\n",
        "Diminishing returns and data limitations make it impractical to train ResNet-152 from scratch on small datasets.\n",
        "\n",
        "Regarding the transferability of features, interesting observations follow:\n",
        "\n",
        "- Despite having frozen the backbone, the final classifier head achieved a test accuracy greater than 84% within just 5 epochs.\n",
        "- Highlighting that the pre-trained features successfully captured the characterstics in our dataset.\n",
        "- This also suggests that the earlier layers in such vision related deep models learn generic features (i.e. edges, textures, strokes, etc.). And the later layers may tend to focus on capturing dataset-specific features.\n",
        "\n",
        "This shows that for vision related tasks, fine-tuning the latter layers is an efficient and stable approach to train such models on differing datasets. Moreover, the pre-trained features are transferable well across vision tasks."
      ],
      "metadata": {
        "id": "T3tXYJCuCr-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual Connections in Practice"
      ],
      "metadata": {
        "id": "bYujXQz6G47z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "from torchvision.models.resnet import Bottleneck\n",
        "\n",
        "# We create a custom bottleneck class to remove residual connections\n",
        "class noSkipBottleneck(Bottleneck):\n",
        "  def forward(self, x: Tensor) -> Tensor:\n",
        "    identity = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv3(out)\n",
        "    out = self.bn3(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "        identity = self.downsample(x)\n",
        "\n",
        "    # out += identity\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "aOfzsWASdpf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate resnet model and modify layers\n",
        "model_noskip_resnet = torchvision.models.resnet152(weights='IMAGENET1K_V1')\n",
        "model_noskip_resnet.fc = nn.Linear(2048, 10)\n",
        "\n",
        "model_noskip_resnet.layer1[2] = noSkipBottleneck(256, 64)\n",
        "model_noskip_resnet.layer2[5] = noSkipBottleneck(512, 128)\n",
        "model_noskip_resnet.layer3[10] = noSkipBottleneck(1024, 256)\n",
        "model_noskip_resnet.layer3[20] = noSkipBottleneck(1024, 256)\n",
        "model_noskip_resnet.layer3[30] = noSkipBottleneck(1024, 256)\n",
        "\n",
        "# print(model_noskip_resnet.layer4)\n",
        "# print(model_resnet.layer3[30])"
      ],
      "metadata": {
        "id": "5NMIY-NGmkUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze backebone except for blocks replaced and classification head\n",
        "\n",
        "for param in model_noskip_resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model_noskip_resnet.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model_noskip_resnet.layer1[2].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model_noskip_resnet.layer2[5].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model_noskip_resnet.layer3[10].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model_noskip_resnet.layer3[20].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model_noskip_resnet.layer3[30].parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "uFPQWYokrZpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now simply train the model\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train(resnet_trainloader, model_noskip_resnet, resnet_loss_fn, resnet_optimizer)\n",
        "    test_loss, test_acc = test(resnet_testloader, model_noskip_resnet, resnet_loss_fn)\n",
        "    print(f\"Epoch {epoch+1}: Train Acc {train_acc:.4f}, Test Acc {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZaBS0owr8IS",
        "outputId": "7dd6818a-c549-4374-c31c-582e162cb96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Acc 0.1004, Test Acc 0.1002\n",
            "Epoch 2: Train Acc 0.1007, Test Acc 0.1018\n",
            "Epoch 3: Train Acc 0.1003, Test Acc 0.1019\n",
            "Epoch 4: Train Acc 0.1008, Test Acc 0.1017\n",
            "Epoch 5: Train Acc 0.1012, Test Acc 0.1001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How do skip connections change gradient flow in very deep networks? What happens to convergence speed and performance when residuals are removed?\n",
        "\n",
        "Offer an alternative, faster path for flow of gradients. Prevents gradient vanishing. Convergence speed and performance are significantly impacted."
      ],
      "metadata": {
        "id": "fdaj0qBX_k-p"
      }
    }
  ]
}